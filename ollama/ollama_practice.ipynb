{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b06684",
   "metadata": {},
   "source": [
    "### Key Features (Ollama):\n",
    "Run models locally – like LLaMA, Mistral, Gemma, Phi, Dolphin (no internet needed).\n",
    "\n",
    "Uses your GPU – if your system has one, it runs faster.\n",
    "\n",
    "Easy commands – works like Docker, very simple for developers.\n",
    "\n",
    "Works with Python – you can use it through API in your Python code.\n",
    "\n",
    "Private & secure – your data stays on your computer, not sent to any server.\n",
    "\n",
    "### How Ollama Works (Internals):\n",
    "You pull a model – Ollama downloads it from its server.\n",
    "\n",
    "The model is saved on your system – it won’t download again.\n",
    "\n",
    "You send a prompt from terminal or Python.\n",
    "\n",
    "Ollama runs the model on your machine and generates a reply.\n",
    "\n",
    "You get the answer via command line or API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b07850",
   "metadata": {},
   "source": [
    "### Install the Ollama: https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861e2bb",
   "metadata": {},
   "source": [
    "| Command                                   | Description                                                     |\n",
    "| ----------------------------------------- | --------------------------------------------------------------- |\n",
    "| `ollama run <model>`                      | Run a model interactively in the terminal                       |\n",
    "| `ollama pull <model>`                     | Download a model (like `llama3`, `mistral`, etc.)               |\n",
    "| `ollama list`                             | Show all models currently installed on your system              |\n",
    "| `ollama show <model>`                     | Show detailed info (parameters, license, modelfile) for a model |\n",
    "| `ollama create <model>`                   | Create a custom model from a Modelfile                          |\n",
    "| `ollama push <model>`                     | Push a custom model to your ollama library (cloud)              |\n",
    "| `ollama serve`                            | Start the local API server                                      |\n",
    "| `ollama run <model> --format json`        | Get responses in JSON format                                    |\n",
    "| `ollama run <model> --system \"<message>\"` | Set a system prompt                                             |\n",
    "| `ollama run <model> --template <file>`    | Use a custom prompt template                                    |\n",
    "| `ollama rm <model>`                       | Remove a model from your local machine                          |\n",
    "| `ollama cp <source> <target>`             | Copy/rename a model                                             |\n",
    "| `ollama pull <model>:<tag>`               | Pull a specific version (e.g., `llama3:latest`)                 |\n",
    "| `ollama run <model> --timeout <seconds>`  | Set timeout for the generation                                  |\n",
    "| `ollama pull llama3 --verbose`              | See detailed download info       |\n",
    "| `ollama run <model> < prompt.txt`           | Feed input from a file           |\n",
    "| `ollama run <model> --context <file>`       | Supply a saved context           |\n",
    "| `ollama run <model> --temperature 0.7`      | Control randomness               |\n",
    "| `ollama run <model> --top-k 50 --top-p 0.9` | Sampling strategies              |\n",
    "| `ollama run <model> --num-predict 100`      | Limit number of tokens generated |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da628f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's dive in.\\n\\nLangChain is an AI model that combines the capabilities of language models and transformer architectures to generate human-like text responses. It's designed to be a general-purpose conversational AI that can understand and respond to natural language inputs, much like a human would.\\n\\nOne of its key features is the ability to contextualize conversations and maintain a coherent narrative across multiple turns. This means LangChain can engage in more sophisticated conversations than simpler chatbots or rule-based systems.\\n\\nWant me to go further into detail?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3:latest\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5215acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama!\n",
      "\n",
      "Ollama is a popular online game that's often described as a mix of Pictionary, Charades, and Hangman. Here's how it works:\n",
      "\n",
      "1. A player thinks of a word, phrase, or concept (the \"Ollama\") and writes a series of squiggles (usually 5-10) to represent the idea.\n",
      "2. The other players try to guess what the Ollama is by suggesting words or phrases that match the squiggles.\n",
      "3. The player who created the Ollama gives hints, such as \"Yes, it's something you find in a kitchen\" or \"No, it's not an animal,\" to help the others guess correctly.\n",
      "\n",
      "The goal is to be the first player to correctly guess what the Ollama is. Sounds simple, but it can get quite challenging and hilarious, especially when players get creative with their squiggles!\n",
      "\n",
      "Ollama has become a beloved online game, with millions of players worldwide. It's a great way to practice communication skills, develop problem-solving strategies, and have fun!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.post(\"http://localhost:11434/api/generate\", json={\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": \"What is Ollama?\",\n",
    "    \"stream\": False\n",
    "})\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0b2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import requests\n",
    "app = FastAPI()\n",
    "@app.get(\"/chat\")\n",
    "def chat(prompt: str):\n",
    "    r = requests.post(\"http://localhost:11434/api/generate\", json={\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    })\n",
    "    return {\"response\": r.json()[\"response\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47126f52",
   "metadata": {},
   "source": [
    "| **Feature**            | **Detail**                               |\n",
    "| ---------------------- | ---------------------------------------- |\n",
    "| **What is Ollama**     | Local LLM runtime                        |\n",
    "| **Installation**       | One-line shell script                    |\n",
    "| **Model usage**        | `ollama run <model>`                     |\n",
    "| **Python integration** | Local REST API                           |\n",
    "| **Ideal for**          | Offline chatbots, privacy-sensitive apps |\n",
    "| **Customization**      | Modelfile-based LLM tweaking             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc4f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7f08b25",
   "metadata": {},
   "source": [
    "| **Platform**       | **Step**                              | **Command**                         | **Description**                              |                                   |\n",
    "| ------------------ | ------------------------------------- | ----------------------------------- | -------------------------------------------- | --------------------------------- |\n",
    "| **Windows**     | All listening ports with PID          | \\`netstat -aon                      | findstr LISTENING\\`                          | Show all listening ports and PIDs |\n",
    "|                    | All ports with process info           | \\`netstat -aon                      | more\\`                                       | Show all active ports             |\n",
    "|                    | Match PID to process name             | `tasklist /FI \"PID eq 12345\"`       | Get process name using PID                   |                                   |\n",
    "| **Linux/macOS** | All listening ports with process info | `sudo lsof -i -P -n \\| grep LISTEN` | Shows port, protocol, and process info       |                                   |\n",
    "|                    | All TCP/UDP sockets with process      | `sudo netstat -tulnp`               | Traditional method                           |                                   |\n",
    "|                    | Fast socket scan                      | `sudo ss -tulwn`                    | Faster than netstat                          |                                   |\n",
    "| **Python**      | Script to list all listening ports    | See code block below                | Use `psutil` to list occupied ports and PIDs |                                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "connections = psutil.net_connections()\n",
    "for conn in connections:\n",
    "    if conn.status == \"LISTEN\":\n",
    "        print(f\"Port: {conn.laddr.port}, PID: {conn.pid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c217e",
   "metadata": {},
   "source": [
    "- tasklist | findstr ollama\n",
    "- netstat -ano | findstr :11434\n",
    "- taskkill /PID <PID_NUMBER> /F\n",
    "- taskkill /PID 12345 /F\n",
    "- ollama serve\n",
    "- OLLAMA_PORT=11435 ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e626004",
   "metadata": {},
   "source": [
    "| **Step**         | **Description**                          | **Command**                           |                  |\n",
    "| ---------------- | ---------------------------------------- | ------------------------------------- | ---------------- |\n",
    "| Step 1        | Check if Ollama is already running       | \\`tasklist                            | findstr ollama\\` |\n",
    "| Step 2 (a)    | Find which process is using port `11434` | \\`netstat -ano                        | findstr :11434\\` |\n",
    "| Step 2 (b)    | Kill that process using its PID          | `taskkill /PID <PID_NUMBER> /F`       |                  |\n",
    "| Example       | Example of killing a process             | `taskkill /PID 12345 /F`              |                  |\n",
    "| Step 3        | Restart Ollama server on default port    | `ollama serve`                        |                  |\n",
    "| Optional Step | Run Ollama server on a different port    | `OLLAMA_PORT=11435 ollama serve`      |                  |\n",
    "| Optional Step | Access new port in code/API              | `http://localhost:11435/api/generate` |                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a07c1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
