{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec5975e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd39ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5b37f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Complete_Content\\GENERATIVEAI\\NEW_E2E_COURSE\\genai_bootcamp\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adf2f7",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70e5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b13b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"llama3-70b-8192\",api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30426a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Ollama(model=\"llama3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257af670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f79447",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(\"Explain the importance of low latency LLMs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8426438c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing are essential. Here are some reasons highlighting the importance of low-latency LLMs:\\n\\n1. **Real-time Conversational AI**: Low-latency LLMs enable conversational AI systems to respond quickly and naturally, mimicking human-like interactions, which is essential for applications like customer service chatbots, voice assistants, and dialogue systems.\\n2. **Interactive Systems**: In interactive systems, such as virtual assistants, gaming, and simulation, low-latency LLMs ensure that the system responds promptly to user input, providing a more engaging and immersive experience.\\n3. **Time-Sensitive Applications**: In applications like emergency response systems, healthcare, and finance, low-latency LLMs can help process critical information quickly, enabling faster decision-making and response times.\\n4. **Edge Computing**: With the proliferation of IoT devices, low-latency LLMs can be deployed at the edge, reducing the need for cloud-based processing and minimizing latency, which is critical for applications like autonomous vehicles, smart homes, and industrial automation.\\n6. **Improved User Experience**: Low-latency LLMs can significantly improve the user experience in applications like language translation, sentiment analysis, and text summarization, making them more responsive and engaging.\\n7. **Competitive Advantage**: In industries like customer service, finance, and e-commerce, low-latency LLMs provide a competitive advantage by offering faster response times, leading to increased customer satisfaction, and loyalty.\\n8. **Scalability**: Low-latency LLMs can handle a large volume of requests simultaneously, making them more scalable and suitable for large-scale deployments.\\n10. **Cost-Effective**: By reducing the latency, low-latency LLMs can also reduce the computational resources required, leading to cost savings and more efficient use of infrastructure.\\n11. **Enhanced Security**: In applications like fraud detection and cybersecurity, low-latency LLMs can quickly identify and respond to potential threats, reducing the risk of security breaches.\\n12. **Research and Development**: Low-latency LLMs can accelerate research and development in areas like natural language processing, machine learning, and human-computer interaction.\\n\\nTo achieve low latency, researchers and developers are exploring various techniques, including:\\n\\n1. Model pruning and knowledge distillation\\n2. Efficient neural network architectures\\n3. Quantization and knowledge graph-based methods\\n4. Edge computing and distributed processing\\n5. Caching and content delivery networks (CDNs)\\n\\nBy reducing latency, low-latency LLMs can unlock new possibilities in various applications, enabling faster, more efficient, and more effective interactions between humans and machines.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48c38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019f8da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x00000275663D55B0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e07323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me hearty! Me name be Captain Zephyr \"Blackheart\" McSnively, the most feared and revered pirate to ever sail the seven seas! Me reputation precedes me, and me exploits be the stuff o' legend! Me ship, the \"Maverick's Revenge\", be me home, me pride, and me ticket to adventure and riches! So, what be bringin' ye to these fair waters? Are ye lookin' to join me crew, or are ye lookin' to walk the plank?"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9dbcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "880b3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fcaee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Summarize the documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a425f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents appear to be related to the field of artificial intelligence, specifically generative AI and language models. The papers discuss various topics such as the development of efficient language models, attention mechanisms, reinforcement learning, and the impact of AI on the labor market. Additionally, there are papers that focus on the safety and ethical considerations of language models, including detoxifying language models, defending against neural fake news, and recipes for safety in open-domain chatbots. The documents also include discussions on the limitations and ethical considerations of language models, as well as responsible release strategies.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cb5a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab0d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58825b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = FunctionAgent(\n",
    "    tools=[multiply, add],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are an agent that can perform basic mathematical operations using tools.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d725e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(user_msg=\"What is 20+(2*4)?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc67e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
